{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE473 Deep Reinforcement Learning Final Project\n",
    "**Author**: Allen Liu\n",
    "**Repository**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "This project is to use the `Proximal Policy Optimization (PPO)` algorithm to learn to navigate a `turtlebot3` in a narrow hallway. The setup of the project is to have `turtlebot3` in a `C` shaped hallway, and the goal is to try to train a model so that it will navigate through the hallway and exit through the exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of the project is shown in the figure below:\n",
    "\n",
    "![](setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, the red robot is the `turtlebot3` that the model is trying to navigate, the blue markers are the walls around the hallway. The goal of the robot is to starting from the origin in the bottom-left corner and to exit from top-right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "In this project, I used the `OpenAI`'s gym for setting up the environment, so that I can use the Deep Reinforcement Learning algorithms to train the model.\n",
    "\n",
    "Since I wrote all dynamics for the `turtlebot3` and the world using `C++` in `ROS2`, I need to write a wrapper class so that it can fit in `OpenAI`'s gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class\n",
    "The environment of the Deep RL model is defined as a subclass of the `OpenAI`'s `gym.Env` class below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "\n",
    "import rclpy.node\n",
    "from tf2_ros import TransformListener, Buffer\n",
    "from tf_transformations import euler_from_quaternion\n",
    "\n",
    "from rcl_interfaces.msg import ParameterDescriptor\n",
    "from std_msgs.msg import Bool\n",
    "from geometry_msgs.msg import Twist\n",
    "from nav_msgs.msg import Odometry\n",
    "\n",
    "from std_srvs.srv import Empty\n",
    "import time\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NuTurtlePPOEnv(gymnasium.Env):\n",
    "    def __init__(self, node: Node) -> None:\n",
    "            super().__init__()\n",
    "            self.node = node\n",
    "            self.action_space = spaces.Box(\n",
    "                low=np.array([0.0, -2.0]),\n",
    "                high=np.array([0.22, 2.0]),\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                dtype=np.float64,\n",
    "                shape=(3,),\n",
    "            )\n",
    "            self.reward_range = (-np.inf, np.inf)\n",
    "            # self.time_\n",
    "            self.odom: Odometry = None\n",
    "            self.state = np.zeros(3)\n",
    "            self.collide = False\n",
    "\n",
    "            self.node.declare_parameter(\n",
    "                \"turn.x\",\n",
    "                3.5,\n",
    "                ParameterDescriptor(description=\"X turning coordinate\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"turn.y\",\n",
    "                3.5,\n",
    "                ParameterDescriptor(description=\"Y turning coordinate\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.d\",\n",
    "                10.0,\n",
    "                ParameterDescriptor(description=\"Gain on distance to desired trajectory\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.target\",\n",
    "                2.5,\n",
    "                ParameterDescriptor(description=\"Gain on target position\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.linear\",\n",
    "                5.0,\n",
    "                ParameterDescriptor(description=\"Gain on linear speed\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.angular\",\n",
    "                1.0,\n",
    "                ParameterDescriptor(description=\"Gain on linear speed\"),\n",
    "            )\n",
    "\n",
    "            self.turn_x = (\n",
    "                self.node.get_parameter(\"turn.x\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.turn_y = (\n",
    "                self.node.get_parameter(\"turn.y\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_d = (\n",
    "                self.node.get_parameter(\"gain.d\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_target = (\n",
    "                self.node.get_parameter(\"gain.target\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_linear = (\n",
    "                self.node.get_parameter(\"gain.linear\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_angular = (\n",
    "                self.node.get_parameter(\"gain.angular\").get_parameter_value().double_value\n",
    "            )\n",
    "\n",
    "            self.pub_cmd_vel = self.node.create_publisher(Twist, \"cmd_vel\", 10)\n",
    "\n",
    "            self.sub_odom = self.node.create_subscription(\n",
    "                Odometry,\n",
    "                \"nusim/odom\",\n",
    "                self.sub_odom_callback,\n",
    "                10,\n",
    "            )\n",
    "            self.sub_collide = self.node.create_subscription(\n",
    "                Bool,\n",
    "                \"nusim/collide\",\n",
    "                self.sub_collide_callback,\n",
    "                10,\n",
    "            )\n",
    "\n",
    "            self.cli_reset_turtle = self.node.create_client(Empty, \"nusim/reset\")\n",
    "\n",
    "            while not self.cli_reset_turtle.wait_for_service(timeout_sec=1.0):\n",
    "                self.node.get_logger().info(\"Service not available, waiting again ...\")\n",
    "                \n",
    "            def sub_odom_callback(self, msg: Odometry):\n",
    "        self.odom = msg\n",
    "        # self.node.get_logger().warn(f\"got odom: {msg.pose.pose.position}\")\n",
    "        # self.node.get_logger().info(f\"State: {self.state}\")\n",
    "\n",
    "    def sub_collide_callback(self, msg: Bool):\n",
    "        if msg.data:\n",
    "            self.collide = True\n",
    "        # self.node.get_logger().info(f\"Collsion: {self.collide}\")\n",
    "\n",
    "    def update_state(self):\n",
    "        if self.odom is None:\n",
    "            self.state = np.zeros(3, dtype=np.float64)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            x = self.odom.pose.pose.position.x\n",
    "            y = self.odom.pose.pose.position.y\n",
    "\n",
    "            qx = self.odom.pose.pose.orientation.x\n",
    "            qy = self.odom.pose.pose.orientation.y\n",
    "            qz = self.odom.pose.pose.orientation.z\n",
    "            qw = self.odom.pose.pose.orientation.w\n",
    "\n",
    "            q = (qx, qy, qz, qw)\n",
    "            e = euler_from_quaternion(q)\n",
    "            # self.node.get_logger().info(f\"euler angle: {e}\")\n",
    "            theta = self.normalize_angle(e[2])\n",
    "\n",
    "            self.state = np.array([x, y, theta])\n",
    "\n",
    "    def normalize_angle(self, angle):\n",
    "        output = np.arctan2(np.sin(angle), np.cos(angle))\n",
    "        return output\n",
    "\n",
    "    def step(self, action):\n",
    "        cmd = Twist()\n",
    "        cmd.linear.x = action[0]\n",
    "        cmd.angular.z = action[1]\n",
    "\n",
    "        self.pub_cmd_vel.publish(cmd)\n",
    "\n",
    "        rclpy.spin_once(node=self.node)\n",
    "        self.update_state()\n",
    "\n",
    "        # reward = self.compute_reward(action)\n",
    "        done = self.is_done()\n",
    "\n",
    "        if done:\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            reward = 1.0\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def compute_reward(self, action):\n",
    "        # self.node.get_logger().info(f\"State: {self.state}\")\n",
    "        x, y, theta = self.state\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        d1 = np.square(y)\n",
    "        d2 = np.square(x - self.turn_x)\n",
    "        d3 = np.square(y - self.turn_y)\n",
    "\n",
    "        if x < 0:\n",
    "            d1 += np.square(x)\n",
    "            d3 += np.square(x)\n",
    "\n",
    "        if x > self.turn_x:\n",
    "            d1 += np.square(x - self.turn_x)\n",
    "            d3 += np.square(x - self.turn_x)\n",
    "\n",
    "        if y < 0:\n",
    "            d2 += np.square(y)\n",
    "\n",
    "        if y > self.turn_y:\n",
    "            d2 += np.square(y - self.turn_y)\n",
    "\n",
    "        d = np.min(np.array([d1, d2, d3]))\n",
    "        min_ind = np.argmin(np.array([d1, d2, d3]))\n",
    "\n",
    "        d_angle = 0\n",
    "        if min_ind == 0:\n",
    "            d_angle = 0\n",
    "\n",
    "        elif min_ind == 1:\n",
    "            d_angle = np.pi / 2.0\n",
    "\n",
    "        elif min_ind == 2:\n",
    "            d_angle = np.pi\n",
    "\n",
    "        e_angle = self.normalize_angle(theta - d_angle)\n",
    "\n",
    "        # reward -= self.gain_d * np.sqrt(d)\n",
    "        # reward -= self.gain_target * np.sum(\n",
    "        #     np.square(self.state[:2] - np.array([0, self.turn_y]))\n",
    "        # )\n",
    "        # reward += self.gain_linear * action[0] + self.gain_angular * np.square(\n",
    "        #     action[1]\n",
    "        # )\n",
    "        if np.sqrt(d) > 0.2:\n",
    "            reward -= 10\n",
    "        else:\n",
    "            reward += scipy.stats.norm.pdf(np.sqrt(d), 0, 0.2)\n",
    "\n",
    "        if np.fabs(e_angle) > 0.5:\n",
    "            reward -= 10\n",
    "        else:\n",
    "            reward += scipy.stats.norm.pdf(np.fabs(e_angle), 0, 0.2)\n",
    "\n",
    "        # self.node.get_logger().info(f\"Reward: {reward}\")\n",
    "        # self.node.get_logger().info(f\"d1: {d1}\")\n",
    "        # self.node.get_logger().info(f\"d2: {d2}\")\n",
    "        # self.node.get_logger().info(f\"d3: {d3}\")\n",
    "        # self.node.get_logger().info(f\"d: {np.sqrt(d)}\")\n",
    "        # self.node.get_logger().info(f\"angle: {e_angle}\")\n",
    "        return reward\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        request = Empty.Request()\n",
    "\n",
    "        future = self.cli_reset_turtle.call_async(request=request)\n",
    "        rclpy.spin_until_future_complete(node=self.node, future=future)\n",
    "\n",
    "        if not future.done():\n",
    "            self.node.get_logger().error(\"ERROR: The service call is not complete\")\n",
    "            raise RuntimeError(\"Service not complete\")\n",
    "\n",
    "        self.state = np.zeros(3, dtype=np.float64)\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.collide:\n",
    "            self.node.get_logger().info(\"Collision\")\n",
    "            self.collide = False\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "In the constructor, it defined all parameters for the environment and set those up as the `ros2` parameters so that it can be passed through `ros2` launch file. Also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kinematics\n",
    "Since the kinematics are calculated through other `ros2` nodes. So for the kinematics update from the action, I will parse the action into the cooresponding `ros2` message to publish and retrieve the updated state via subscribed message.\n",
    "\n",
    "The message published is the `cmd_vel`, which is type of `geometry_msgs/Twist`. This message contains both linear and angular velocity so that it can be used for commanding the `turtlebot3` to move at the commanded speed.\n",
    "\n",
    "To retrieve the updated state of the `turtlebot3`, since this is the simulation, so I assume the perfect odometry. So that I can retreve the updated state by subcribing the `odom` message, which is type `nav_msgs/Odometry`, which contains the position $x$, $y$, $z$, and orientation $\\phi$, $\\theta$ and $\\psi$ (`roll`, `pitch` and `yaw`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
