{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE473 Deep Reinforcement Learning Final Project\n",
    " - **Author**: Allen Liu\n",
    " - **Repository**: [GitHub Repo](https://github.com/nu-jliu/deep_reinforce_learning_navigation/tree/main)\n",
    " - **Youtube URL**: [YouTube Video](https://youtu.be/9-lRs6uwU5k)\n",
    "\n",
    " <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9-lRs6uwU5k?si=Jne5G9m4TYuTavLN\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "This project is to use the `Proximal Policy Optimization (PPO)` and `Deep Q Network` algorithm to learn to navigate a `turtlebot3` in a narrow hallway. The setup of the project is to have `turtlebot3` in a `C` shaped hallway, and the goal is to try to train a model so that it will navigate through the hallway and exit through the exit. And goal of this project is to compare the effectiveness of two algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of the project is shown in the figure below:\n",
    "\n",
    "![](setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, the red robot is the `turtlebot3` that the model is trying to navigate, the blue markers are the walls around the hallway. The goal of the robot is to starting from the origin in the bottom-left corner and to exit from top-right corner.\n",
    "\n",
    "The structure of this project is shown below:\n",
    "\n",
    "![](rosgraph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "In this project, I used the `OpenAI`'s gym for setting up the environment, so that I can use the Deep Reinforcement Learning algorithms to train the model.\n",
    "\n",
    "Since I wrote all dynamics for the `turtlebot3` and the world using `C++` in `ROS2`, I need to write a wrapper class so that it can fit in `OpenAI`'s gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class\n",
    "The environment of the Deep RL model is defined as a subclass of the `OpenAI`'s `gym.Env` class below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from typing import Optional\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "\n",
    "from tf_transformations import euler_from_quaternion\n",
    "\n",
    "from rcl_interfaces.msg import ParameterDescriptor\n",
    "from std_msgs.msg import Bool\n",
    "from geometry_msgs.msg import Twist\n",
    "from nav_msgs.msg import Odometry\n",
    "\n",
    "from std_srvs.srv import Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuTurtlePPOEnv(gymnasium.Env):\n",
    "    def __init__(self, node: Node) -> None:\n",
    "            super().__init__()\n",
    "            self.node = node\n",
    "            self.action_space = spaces.Box(\n",
    "                low=np.array([0.0, -2.0]),\n",
    "                high=np.array([0.22, 2.0]),\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                dtype=np.float64,\n",
    "                shape=(3,),\n",
    "            )\n",
    "            self.reward_range = (-np.inf, np.inf)\n",
    "            # self.time_\n",
    "            self.odom: Odometry = None\n",
    "            self.state = np.zeros(3)\n",
    "            self.collide = False\n",
    "\n",
    "            self.node.declare_parameter(\n",
    "                \"turn.x\",\n",
    "                3.5,\n",
    "                ParameterDescriptor(description=\"X turning coordinate\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"turn.y\",\n",
    "                3.5,\n",
    "                ParameterDescriptor(description=\"Y turning coordinate\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.d\",\n",
    "                10.0,\n",
    "                ParameterDescriptor(description=\"Gain on distance to desired trajectory\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.target\",\n",
    "                2.5,\n",
    "                ParameterDescriptor(description=\"Gain on target position\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.linear\",\n",
    "                5.0,\n",
    "                ParameterDescriptor(description=\"Gain on linear speed\"),\n",
    "            )\n",
    "            self.node.declare_parameter(\n",
    "                \"gain.angular\",\n",
    "                1.0,\n",
    "                ParameterDescriptor(description=\"Gain on linear speed\"),\n",
    "            )\n",
    "\n",
    "            self.turn_x = (\n",
    "                self.node.get_parameter(\"turn.x\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.turn_y = (\n",
    "                self.node.get_parameter(\"turn.y\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_d = (\n",
    "                self.node.get_parameter(\"gain.d\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_target = (\n",
    "                self.node.get_parameter(\"gain.target\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_linear = (\n",
    "                self.node.get_parameter(\"gain.linear\").get_parameter_value().double_value\n",
    "            )\n",
    "            self.gain_angular = (\n",
    "                self.node.get_parameter(\"gain.angular\").get_parameter_value().double_value\n",
    "            )\n",
    "\n",
    "            self.pub_cmd_vel = self.node.create_publisher(Twist, \"cmd_vel\", 10)\n",
    "\n",
    "            self.sub_odom = self.node.create_subscription(\n",
    "                Odometry,\n",
    "                \"nusim/odom\",\n",
    "                self.sub_odom_callback,\n",
    "                10,\n",
    "            )\n",
    "            self.sub_collide = self.node.create_subscription(\n",
    "                Bool,\n",
    "                \"nusim/collide\",\n",
    "                self.sub_collide_callback,\n",
    "                10,\n",
    "            )\n",
    "\n",
    "            self.cli_reset_turtle = self.node.create_client(Empty, \"nusim/reset\")\n",
    "\n",
    "            while not self.cli_reset_turtle.wait_for_service(timeout_sec=1.0):\n",
    "                self.node.get_logger().info(\"Service not available, waiting again ...\")\n",
    "                \n",
    "    def sub_odom_callback(self, msg: Odometry):\n",
    "        self.odom = msg\n",
    "        # self.node.get_logger().warn(f\"got odom: {msg.pose.pose.position}\")\n",
    "        # self.node.get_logger().info(f\"State: {self.state}\")\n",
    "\n",
    "    def sub_collide_callback(self, msg: Bool):\n",
    "        if msg.data:\n",
    "            self.collide = True\n",
    "        # self.node.get_logger().info(f\"Collsion: {self.collide}\")\n",
    "\n",
    "    def update_state(self):\n",
    "        if self.odom is None:\n",
    "            self.state = np.zeros(3, dtype=np.float64)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            x = self.odom.pose.pose.position.x\n",
    "            y = self.odom.pose.pose.position.y\n",
    "\n",
    "            qx = self.odom.pose.pose.orientation.x\n",
    "            qy = self.odom.pose.pose.orientation.y\n",
    "            qz = self.odom.pose.pose.orientation.z\n",
    "            qw = self.odom.pose.pose.orientation.w\n",
    "\n",
    "            q = (qx, qy, qz, qw)\n",
    "            e = euler_from_quaternion(q)\n",
    "            # self.node.get_logger().info(f\"euler angle: {e}\")\n",
    "            theta = self.normalize_angle(e[2])\n",
    "\n",
    "            self.state = np.array([x, y, theta])\n",
    "\n",
    "    def normalize_angle(self, angle):\n",
    "        output = np.arctan2(np.sin(angle), np.cos(angle))\n",
    "        return output\n",
    "\n",
    "    def step(self, action):\n",
    "        cmd = Twist()\n",
    "        cmd.linear.x = action[0]\n",
    "        cmd.angular.z = action[1]\n",
    "\n",
    "        self.pub_cmd_vel.publish(cmd)\n",
    "\n",
    "        rclpy.spin_once(node=self.node)\n",
    "        self.update_state()\n",
    "\n",
    "        done = self.is_done()\n",
    "        reward = self.compute_reward(done)\n",
    "\n",
    "        if done:\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            reward = 1.0\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def compute_reward(self, done):\n",
    "        x, y, _ = self.state\n",
    "        d = np.sqrt(np.square(x) + np.square(y - 4.0))\n",
    "\n",
    "        if done:\n",
    "            return -1.0\n",
    "\n",
    "        elif d < 0.5:\n",
    "            return 1.0\n",
    "\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        request = Empty.Request()\n",
    "\n",
    "        future = self.cli_reset_turtle.call_async(request=request)\n",
    "        rclpy.spin_until_future_complete(node=self.node, future=future)\n",
    "\n",
    "        if not future.done():\n",
    "            self.node.get_logger().error(\"ERROR: The service call is not complete\")\n",
    "            raise RuntimeError(\"Service not complete\")\n",
    "\n",
    "        self.state = np.zeros(3, dtype=np.float64)\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.collide:\n",
    "            self.node.get_logger().info(\"Collision\")\n",
    "            self.collide = False\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "In the constructor, it defined all parameters for the environment and set those up as the `ros2` parameters so that it can be passed through `ros2` launch file. Also is creates corresponding subscriber and publishers so that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kinematics Update\n",
    "Since the kinematics are calculated through other `ros2` nodes. So for the kinematics update from the action, I will parse the action into the cooresponding `ros2` message to publish and retrieve the updated state via subscribed message.\n",
    "\n",
    "The message published is the `cmd_vel`, which is type of `geometry_msgs/Twist`. This message contains both linear and angular velocity so that it can be used for commanding the `turtlebot3` to move at the commanded speed.\n",
    "\n",
    "To retrieve the updated state of the `turtlebot3`, since this is the simulation, so I assume the perfect odometry. So that I can retreve the updated state by subcribing the `odom` message, which is type `nav_msgs/Odometry`, which contains the position $x$, $y$, $z$, and orientation $\\phi$, $\\theta$ and $\\psi$ (`roll`, `pitch` and `yaw`). So that I can get most recent robot's state by subcribing from odometry message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Calculation\n",
    "The `nusim` node running simulation for `turtlebot3` will publish a message indicating whether the robot has collided with the wall. If it is collided, it will return a reward as `-1.0`, if it is still exploring, it will return a reward as `0.0`, when it succeed, it will return a reward as `1.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Termination\n",
    "The environment will subscribe to the `collide` message from the `nusim` node, so that when the robot collides with the wall, the node will publish a message when it collides and it will trigger the termination of the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting Environment\n",
    "To reset to `turtlebot3` simulation environment, it has to call the `reset` service with the type of `std_srvs/Empty` served by the `nusim` node. So whenever the current node has died, it needs to be respawned. The reset will be called so that the nusim node will reset the `turtlebot3`'s environment, and restart the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Training\n",
    "There is a `python` node that used for initiate the training process, that create a environment using the environment wrapper class and then use one of the algorithm: `PPO` or `DQN` to train the model. This node will use either `PPO` or `DQN` algorithm to train a model under the predefined environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is shown in the figure below.\n",
    "\n",
    "![](training.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed implementation of the training node is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "\n",
    "from rcl_interfaces.msg import ParameterDescriptor\n",
    "\n",
    "from nuturtle_deep_rl.nuturtle_ppo_env import NuTurtlePPOEnv\n",
    "from nuturtle_deep_rl.nuturtle_dqn_env import NuTurtleDQNEnv\n",
    "\n",
    "\n",
    "class NuTurtleDRL(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"nuturtle_deeprl\")\n",
    "\n",
    "        # ---------- ROS Setup ----------\n",
    "        self.declare_parameter(\n",
    "            \"pkg_share_dir\",\n",
    "            \"\",\n",
    "            ParameterDescriptor(description=\"Package share directory\"),\n",
    "        )\n",
    "        self.declare_parameter(\n",
    "            \"log_dirname\",\n",
    "            \"turtlebot3.log\",\n",
    "            ParameterDescriptor(description=\"Log directory\"),\n",
    "        )\n",
    "        self.declare_parameter(\n",
    "            \"algorithm\",\n",
    "            \"ppo\",\n",
    "            ParameterDescriptor(\n",
    "                description=\"Algorithm used for deep reinforcement learning\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.pkg_share_dir = (\n",
    "            self.get_parameter(\"pkg_share_dir\").get_parameter_value().string_value\n",
    "        )\n",
    "        self.log_dirname = (\n",
    "            self.get_parameter(\"log_dirname\").get_parameter_value().string_value\n",
    "        )\n",
    "        self.algorithm = (\n",
    "            self.get_parameter(\"algorithm\").get_parameter_value().string_value\n",
    "        )\n",
    "\n",
    "        self.log_path = os.path.join(\n",
    "            self.pkg_share_dir,\n",
    "            f\"{self.algorithm}_{self.log_dirname}\",\n",
    "        )\n",
    "        self.get_logger().info(\"Setting up environment ...\")\n",
    "\n",
    "        if self.algorithm == \"dqn\":\n",
    "            self.env_dqn = NuTurtleDQNEnv(node=self)\n",
    "            self.model = DQN(\n",
    "                policy=\"MlpPolicy\",\n",
    "                env=self.env_dqn,\n",
    "                verbose=1,\n",
    "                policy_kwargs=dict(net_arch=[64, 64]),\n",
    "                tensorboard_log=self.log_path,\n",
    "            )\n",
    "\n",
    "            net = self.model.q_net\n",
    "            for name, layer in net.named_children():\n",
    "                self.get_logger().info(f\"{name}: {layer}\")\n",
    "\n",
    "        elif self.algorithm == \"ppo\":\n",
    "            self.env_ppo = NuTurtlePPOEnv(node=self)\n",
    "            self.model = PPO(\n",
    "                policy=\"MlpPolicy\",\n",
    "                env=self.env_ppo,\n",
    "                verbose=1,\n",
    "                policy_kwargs=dict(net_arch=[64, 64]),\n",
    "                tensorboard_log=self.log_path,\n",
    "            )\n",
    "\n",
    "            net = self.model.policy.action_net\n",
    "\n",
    "            for name, layer in net.named_children():\n",
    "                self.get_logger().info(f\"{name}, {layer}\")\n",
    "\n",
    "        self.get_logger().warn(\"Start training ...\")\n",
    "        self.model.learn(total_timesteps=int(1e15), progress_bar=True, log_interval=1)\n",
    "        self.model.save(\n",
    "            os.path.join(\n",
    "                self.pkg_share_dir,\n",
    "                f\"{self.algorithm}_turtlebot3\",\n",
    "            )\n",
    "        )\n",
    "        self.get_logger().info(\"Training finished\")\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node_drl = NuTurtleDRL()\n",
    "    rclpy.spin(node=node_drl)\n",
    "\n",
    "    node_drl.destroy_node()\n",
    "    rclpy.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this node, it uses the `stable_baseline3` library to apply either `PPO` or `DQN` algorithm using the customed environment, then it will try to train a model using the defined environment with the desired algorithm. As result the `DQN` algorithm performs much better than the `PPO` algorithm based on the  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Performance\n",
    "The `DQN` performs better than `PPO` since the `DQN` converges within 30 minutes, but the `PPO` never converge.\n",
    "\n",
    "### Result\n",
    "The result for `PPO` was far away from the expected but `DQN` is closer to the expected one.\n",
    "\n",
    "### Parameter\n",
    "To get better result, we need to adjust the way to calculate the reward function so that during the training process, it is more likely to find the solution. Also when constructing the model, we can adjust the parameters such as `learning_rate`, `gamma`, `n_batch`, `n_epoch` as well as the network structure.\n",
    "\n",
    "### Parameter adjusted\n",
    "During this project, I adjusted the `n_batch`, `learning_rate`, `reward function` as well as the network structure. Based on the result, the `reward function` plays the most important role during the learning process, and has the most impact over accuracy of the entire system.\n",
    "\n",
    "### Challenges\n",
    "The most challenge I faced during this project is the hardware limitation. Since this environment is computationally heavy, and there are a great number of processes running for each learning session. Even though I am only running 10 learning sessions, I have utilized all 32 threads along with almost 70% of GPU utilization as well as 26G of memory access. So that sometimes, the system will get out of memmory and just crashed.\n",
    "\n",
    "### Future Work\n",
    "In the future, I will try to get a optimal reward function to have it converge within shorter period. Also I plan to optimize the code, so that it will use less system resources while running, which can help to boost the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After finishing this project, it can be concluded that `DQN` algorithm is much more reliable than the `PPO` algorithm since it is more likely to converge with the specified time. Also based on the learning behavior, the `DQN` algorithm is more likely to choose the path that moves along the desired trajectory compared to `PPO` algorithm, hence `DQN` is more reliable than `PPO`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
